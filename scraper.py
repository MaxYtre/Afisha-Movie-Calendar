#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ü–∞—Ä—Å–µ—Ä –∞—Ñ–∏—à–∏ –∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä–æ–≤ –ü–µ—Ä–º–∏ —Å afisha.ru
–°–æ–∑–¥–∞—ë—Ç all-day —Å–æ–±—ã—Ç–∏—è –±–µ–∑ –≤—Ä–µ–º–µ–Ω–∏ –∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π —Å –∫—Ä–∞—Å–∏–≤—ã–º–∏ –æ–ø–∏—Å–∞–Ω–∏—è–º–∏
–° —Ö–ª–æ–ø—É—à–∫–∞–º–∏ üëè –ø–µ—Ä–µ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º, –±–∞–Ω–Ω–µ—Ä–∞–º–∏, —Ä–µ–π—Ç–∏–Ω–≥–∞–º–∏ –∏ –æ–ø–∏—Å–∞–Ω–∏—è–º–∏ —Ñ–∏–ª—å–º–æ–≤
"""

import argparse
import logging
import os
import random
import re
import time
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup
from icalendar import Calendar, Event

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
BASE_URL = 'https://www.afisha.ru/prm/schedule_cinema/'
SCHEDULE_URL = BASE_URL  # –°—Ç—Ä–∞–Ω–∏—Ü–∞ 1
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
HEADERS = {
    'User-Agent': USER_AGENT,
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Sec-Fetch-Dest': 'document',
    'Sec-Fetch-Mode': 'navigate',
    'Sec-Fetch-Site': 'none',
    'Cache-Control': 'max-age=0',
    'Upgrade-Insecure-Requests': '1'
}

# –ó–∞–¥–µ—Ä–∂–∫–∏ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è HTTP 429
DELAYS = {
    'default': 5,    # –û–±—ã—á–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    'page': 8,       # –°—Ç—Ä–∞–Ω–∏—Ü—ã —Å–ø–∏—Å–∫–æ–≤
    'detail': 12,    # –î–µ—Ç–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ñ–∏–ª—å–º–æ–≤
    'retry': 10      # –ü–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏
}

def smart_delay(delay_type: str = 'default', multiplier: int = 1):
    """–£–º–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ —Å —Å–ª—É—á–∞–π–Ω–æ–π –≤–∞—Ä–∏–∞—Ü–∏–µ–π"""
    base_delay = DELAYS.get(delay_type, DELAYS['default'])
    delay = base_delay * multiplier + random.uniform(0, 3)
    time.sleep(delay)
    logger.debug(f"–ó–∞–¥–µ—Ä–∂–∫–∞ {delay_type}: {delay:.2f} —Å–µ–∫")

def make_request(session: requests.Session, url: str, delay_type: str = 'default') -> Optional[requests.Response]:
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç HTTP-–∑–∞–ø—Ä–æ—Å —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –∏ retry"""
    smart_delay(delay_type)
    try:
        response = session.get(url, headers=HEADERS, timeout=30)
        if response.status_code == 429:
            logger.warning(f"HTTP 429 –¥–ª—è {url}. –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É.")
            time.sleep(60)  # –î–ª–∏–Ω–Ω–∞—è –ø–∞—É–∑–∞ –ø—Ä–∏ rate limit
            return make_request(session, url, 'retry')
        response.raise_for_status()
        return response
    except requests.RequestException as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ {url}: {e}")
        return None

def parse_schedule_calendar(soup: BeautifulSoup) -> Optional[datetime]:
    """–ü–∞—Ä—Å–∏—Ç –∫–∞–ª–µ–Ω–¥–∞—Ä—å –≤–∏–¥–∂–µ—Ç–∞ –¥–ª—è –±–ª–∏–∂–∞–π—à–µ–π –¥–∞—Ç—ã —Å–µ–∞–Ω—Å–∞ (–Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ HTML)"""
    calendar_div = soup.find('div', {'aria-label': '–ö–∞–ª–µ–Ω–¥–∞—Ä—å'})
    if not calendar_div:
        logger.warning("–ö–∞–ª–µ–Ω–¥–∞—Ä—å –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return None

    # –ò—â–µ–º –∞–∫—Ç–∏–≤–Ω—ã–µ –¥–∞—Ç—ã (–∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–µ <a class="pdT6c">)
    active_days = calendar_div.find_all('a', class_=re.compile(r'pdT6c'))
    if not active_days:
        logger.warning("–ê–∫—Ç–∏–≤–Ω—ã–µ –¥–∞—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return None

    # –ë–ª–∏–∂–∞–π—à–∞—è –¥–∞—Ç–∞ - –ø–µ—Ä–≤–∞—è –∞–∫—Ç–∏–≤–Ω–∞—è
    first_day = active_days[0]
    aria_label = first_day.get('aria-label', '')
    # –ü–∞—Ä—Å–∏–º –∏–∑ aria-label, –Ω–∞–ø—Ä. "8 –æ–∫—Ç—è–±—Ä—è"
    date_match = re.search(r'(\d+)\s+([–∞-—è]+)', aria_label.lower())
    if date_match:
        day = int(date_match.group(1))
        month_name = date_match.group(2)
        month_map = {
            '–æ–∫—Ç—è–±—Ä—è': 10, '–Ω–æ—è–±—Ä—è': 11, '–¥–µ–∫–∞–±—Ä—è': 12,
            '—è–Ω–≤–∞—Ä—è': 1, '—Ñ–µ–≤—Ä–∞–ª—è': 2, '–º–∞—Ä—Ç–∞': 3, '–∞–ø—Ä–µ–ª—è': 4, '–º–∞—è': 5,
            '–∏—é–Ω—è': 6, '–∏—é–ª—è': 7, '–∞–≤–≥—É—Å—Ç–∞': 8, '—Å–µ–Ω—Ç—è–±—Ä—è': 9
        }
        if month_name in month_map:
            current_year = datetime.now().year
            try:
                dt = datetime(current_year, month_map[month_name], day)
                logger.info(f"–ë–ª–∏–∂–∞–π—à–∞—è –¥–∞—Ç–∞ —Å–µ–∞–Ω—Å–∞: {dt.strftime('%d.%m.%Y')}")
                return dt.date()
            except ValueError:
                pass

    # Fallback: –∑–∞–≤—Ç—Ä–∞
    tomorrow = (datetime.now() + timedelta(days=1)).date()
    logger.info(f"Fallback –¥–∞—Ç–∞: –∑–∞–≤—Ç—Ä–∞ {tomorrow}")
    return tomorrow

def extract_movie_detail(session: requests.Session, movie_url: str) -> Dict[str, Any]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–µ—Ç–∞–ª–∏ —Ñ–∏–ª—å–º–∞: –±–∞–Ω–Ω–µ—Ä, –æ–ø–∏—Å–∞–Ω–∏–µ, –≤–æ–∑—Ä–∞—Å—Ç, —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ, —Å—Ç—Ä–∞–Ω–∞"""
    response = make_request(session, movie_url, 'detail')
    if not response:
        return {}

    soup = BeautifulSoup(response.text, 'html.parser')
    details = {}

    # –ë–∞–Ω–Ω–µ—Ä (img src –∏–ª–∏ background-image)
    banner_img = soup.find('img', class_=re.compile(r'(poster|banner|hero-image)')) or soup.find('img', alt=re.compile(r'.*—Ñ–∏–ª—å–º.*', re.I))
    if banner_img:
        src = banner_img.get('src') or banner_img.get('data-src')
        banner_url = urljoin(BASE_URL, src) if src else ''
        if not banner_url:
            # –ï—Å–ª–∏ background-image
            style = banner_img.get('style', '')
            bg_match = re.search(r'url\(["\']?(https?://[^"\']+)["\']?\)', style)
            if bg_match:
                banner_url = bg_match.group(1)
        if banner_url:
            details['banner'] = banner_url
            logger.debug(f"–ë–∞–Ω–Ω–µ—Ä: {banner_url}")

    # –í–æ–∑—Ä–∞—Å—Ç (12+, 16+)
    age_elem = soup.find('span', class_=re.compile(r'age|rating')) or soup.find(string=re.compile(r'\d+\+'))
    if age_elem:
        age_text = re.search(r'(\d+)\+', str(age_elem))
        if age_text:
            details['age'] = age_text.group(1) + '+'
            logger.debug(f"–í–æ–∑—Ä–∞—Å—Ç: {details['age']}")

    # –û–ø–∏—Å–∞–Ω–∏–µ –ø–æ–¥ "–û —Ñ–∏–ª—å–º–µ"
    desc_section = soup.find('h2', string=re.compile(r'–æ —Ñ–∏–ª—å–º–µ', re.I))
    if desc_section:
        desc_elem = desc_section.find_next_sibling('div', class_=re.compile(r'description|about')) or desc_section.find_next('p')
        if desc_elem:
            description = desc_elem.get_text(strip=True)[:300]
            if len(description) > 200:
                description += '...'
            details['description'] = description
            logger.debug(f"–û–ø–∏—Å–∞–Ω–∏–µ: {description[:50]}...")

    # –†–∞—Å–ø–∏—Å–∞–Ω–∏–µ (–±–ª–∏–∂–∞–π—à–∞—è –¥–∞—Ç–∞)
    schedule_date = parse_schedule_calendar(soup)
    if schedule_date:
        details['date'] = schedule_date

    # –°—Ç—Ä–∞–Ω–∞ (–¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è –†–æ—Å—Å–∏–∏)
    country_elem = soup.find(string=re.compile(r'(—Ä–æ—Å—Å–∏|usa|uk|france|germany)', re.I))
    if country_elem:
        details['country'] = country_elem.strip()[:50]  # –ö–æ—Ä–æ—Ç–∫–æ

    return details

def extract_movie_from_list(soup: BeautifulSoup) -> List[Dict[str, Any]]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ñ–∏–ª—å–º—ã —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–ø–∏—Å–∫–∞"""
    movies = []
    movie_elements = soup.find_all('div', class_=re.compile(r'movie|film|item|card'))
    for elem in movie_elements:
        title_elem = elem.find('a', class_=re.compile(r'title|name|h3')) or elem.find('h3')
        if title_elem:
            title = title_elem.get_text(strip=True)
            url = urljoin(BASE_URL, title_elem.get('href', ''))
            if title and url:
                movie = {
                    'title': title,
                    'url': url,
                    'date': None,
                    'time': None  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
                }
                movies.append(movie)
                logger.debug(f"–ù–∞–π–¥–µ–Ω —Ñ–∏–ª—å–º: {title}")

    logger.info(f"–ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –Ω–∞–π–¥–µ–Ω–æ {len(movies)} —Ñ–∏–ª—å–º–æ–≤")
    return movies

def parse_all_schedule_pages(session: requests.Session, max_pages: Optional[int] = None) -> List[Dict[str, Any]]:
    """–ü–∞—Ä—Å–∏—Ç –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–∞–≥–∏–Ω–∞—Ü–∏–∏"""
    all_movies = []
    existing_titles = set()
    current_page = 1
    page_count = 0

    while True:
        if max_pages and page_count >= max_pages:
            logger.info(f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü: {max_pages}")
            break

        page_url = SCHEDULE_URL if current_page == 1 else f"{BASE_URL}page{current_page}/"
        logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {current_page}: {page_url}")

        response = make_request(session, page_url, 'page')
        if not response:
            logger.warning(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {current_page} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ - –∑–∞–≤–µ—Ä—à–∞–µ–º")
            break

        soup = BeautifulSoup(response.text, 'html.parser')
        page_movies = extract_movie_from_list(soup)

        if not page_movies:
            logger.info(f"–ü—É—Å—Ç–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {current_page} - –∑–∞–≤–µ—Ä—à–∞–µ–º")
            break

        for movie in page_movies:
            if movie['title'] not in existing_titles:
                existing_titles.add(movie['title'])
                all_movies.append(movie)

        page_count += 1
        current_page += 1

    logger.info(f"–ò—Ç–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ–∏–ª—å–º–æ–≤: {len(all_movies)} –Ω–∞ {page_count} —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö")
    return all_movies

def create_event(movie: Dict[str, Any], details: Dict[str, Any], exclude_country: str = '–†–æ—Å—Å–∏—è') -> Optional[Event]:
    """–°–æ–∑–¥–∞—ë—Ç all-day —Å–æ–±—ã—Ç–∏–µ –±–µ–∑ –≤—Ä–µ–º–µ–Ω–∏ –∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π —Å –∫—Ä–∞—Å–∏–≤—ã–º –æ–ø–∏—Å–∞–Ω–∏–µ–º"""
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä–∞–Ω—ã
    if 'country' in details and exclude_country.lower() in details['country'].lower():
        logger.info(f"–ü—Ä–æ–ø—É—Å–∫ {exclude_country} —Ñ–∏–ª—å–º–∞: {movie['title']}")
        return None

    # –î–∞—Ç–∞
    event_date = details.get('date', (datetime.now() + timedelta(days=1)).date())
    event_date_str = event_date.strftime('%Y%m%d')

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å —Ö–ª–æ–ø—É—à–∫–æ–π
    summary = f"üëè {movie['title']}"

    # –ö—Ä–∞—Å–∏–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
    description_parts = [f"üé≠ –†–µ–π—Ç–∏–Ω–≥: {details.get('age', '–ù–µ —É–∫–∞–∑–∞–Ω')}"]
    if 'banner' in details:
        description_parts.append(f"\n[![–ë–∞–Ω–Ω–µ—Ä —Ñ–∏–ª—å–º–∞]]({details['banner']})")
    else:
        description_parts.append("\n[–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ]")
    if 'description' in details:
        description_parts.append(f"\nüìú –û —Ñ–∏–ª—å–º–µ:\n{details['description']}")
    else:
        description_parts.append("\nüìú –û —Ñ–∏–ª—å–º–µ:\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
    description_parts.append("\nüóìÔ∏è –°–æ–±—ã—Ç–∏–µ –Ω–∞ –≤–µ—Å—å –¥–µ–Ω—å: —Ñ–∏–ª—å–º –≤ –∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä–∞—Ö –ü–µ—Ä–º–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∞–∫—Ç—É–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ afisha.ru.")
    description_parts.append(f"\nüìç –ò—Å—Ç–æ—á–Ω–∏–∫: {movie['url']}")
    description = '\n'.join(description_parts)

    # All-day —Å–æ–±—ã—Ç–∏–µ –±–µ–∑ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π
    event = Event()
    event.add('uid', f"afisha-movie-{hash(movie['title'])}@maxytre.github.io")
    event.add('summary', summary)
    event.add('dtstart', datetime.strptime(event_date_str, '%Y%m%d').date())  # All-day
    event.add('dtend', datetime.strptime(event_date_str, '%Y%m%d').date() + timedelta(days=1))
    event.add('description', description)
    event.add('location', '–ö–∏–Ω–æ—Ç–µ–∞—Ç—Ä—ã –ü–µ—Ä–º–∏')
    # –ë–µ–∑ VALARM

    logger.info(f"–°–æ–∑–¥–∞–Ω–æ —Å–æ–±—ã—Ç–∏–µ: {summary} –Ω–∞ {event_date}")
    return event

def main():
    parser = argparse.ArgumentParser(description="–ü–∞—Ä—Å–µ—Ä –∞—Ñ–∏—à–∏ –∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä–æ–≤ –ü–µ—Ä–º–∏")
    parser.add_argument('--exclude-country', default='–†–æ—Å—Å–∏—è', help='–ò—Å–∫–ª—é—á–∏—Ç—å —Ñ–∏–ª—å–º—ã —Å—Ç—Ä–∞–Ω—ã')
    parser.add_argument('--delay', type=int, default=5, help='–ë–∞–∑–æ–≤–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ (—Å–µ–∫)')
    parser.add_argument('--skip-details', action='store_true', help='–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –¥–µ—Ç–∞–ª–∏')
    parser.add_argument('--max-movies', type=int, default=None, help='–ú–∞–∫—Å. —Ñ–∏–ª—å–º–æ–≤')
    parser.add_argument('--max-pages', type=int, default=None, help='–ú–∞–∫—Å. —Å—Ç—Ä–∞–Ω–∏—Ü')

    args = parser.parse_args()

    # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–¥–µ—Ä–∂–µ–∫
    scale = args.delay / 5.0
    for key in DELAYS:
        DELAYS[key] *= scale

    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞: all-day —Å–æ–±—ã—Ç–∏—è –±–µ–∑ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π —Å —Ö–ª–æ–ø—É—à–∫–∞–º–∏ üëè")
    logger.info(f"–†–µ–∂–∏–º: {'--skip-details' if args.skip_details else '–ü–æ–ª–Ω—ã–π'}")
    if args.max_movies:
        logger.info(f"–õ–∏–º–∏—Ç —Ñ–∏–ª—å–º–æ–≤: {args.max_movies}")
    if args.max_pages:
        logger.info(f"–õ–∏–º–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü: {args.max_pages}")

    session = requests.Session()
    session.headers.update(HEADERS)

    all_movies = parse_all_schedule_pages(session, args.max_pages)
    if not all_movies:
        logger.error("–ù–µ—Ç —Ñ–∏–ª—å–º–æ–≤")
        return

    if args.max_movies:
        all_movies = all_movies[:args.max_movies]
        logger.info(f"–û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –¥–æ {len(all_movies)} —Ñ–∏–ª—å–º–æ–≤")

    events = []
    processed = 0
    for movie in all_movies:
        details = {}
        if not args.skip_details:
            details = extract_movie_detail(session, movie['url'])
        event = create_event(movie, details, args.exclude_country)
        if event:
            events.append(event)
        processed += 1
        if processed % 10 == 0:
            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed}/{len(all_movies)} —Ñ–∏–ª—å–º–æ–≤, {len(events)} —Å–æ–±—ã—Ç–∏–π")
        if not args.skip_details:
            smart_delay('detail', 0.5)

    # –ö–∞–ª–µ–Ω–¥–∞—Ä—å
    cal = Calendar()
    cal.add('prodid', '-//Afisha Calendar//MaxYtre//RU')
    cal.add('version', '2.0')
    cal.add('calscale', 'GREGORIAN')
    cal.add('method', 'PUBLISH')
    for event in events:
        cal.add_component(event)

    with open('calendar.ics', 'wb') as f:
        f.write(cal.to_ical())

    logger.info(f"‚úÖ –ì–æ—Ç–æ–≤–æ: {len(events)} —Å–æ–±—ã—Ç–∏–π –≤ calendar.ics (all-day –±–µ–∑ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π)")

if __name__ == '__main__':
    main()
